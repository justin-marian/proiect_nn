{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdac019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from voc import get_dataloader\n",
    "from main_utils import set_seed\n",
    "from model_factory import get_model\n",
    "from ema import RobustEMA\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31451bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = (224, 224)\n",
    "CONFIDENCE_THRESHOLD = 0.7\n",
    "BATCH_SIZE = 4\n",
    "CHECKPOINT_DIR = \"./checkpoints\"\n",
    "METRIC_SUPERVISED = [\"loss_classifier\", \"loss_box_reg\", \"loss_objectness\", \"loss_rpn_box_reg\"]\n",
    "METRICS_UNSUPERVISED = [\"loss_classifier\", \"loss_objectness\"]\n",
    "LAMBDA_UNSUPERVISED = 5.0\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "def scale_to_01(image, **kwargs):\n",
    "    return image.astype('float32') / 255.0\n",
    "\n",
    "weak_augmentations = A.Compose([\n",
    "    A.Resize(SIZE[0], SIZE[1]),         \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Lambda(image=scale_to_01), \n",
    "    ToTensorV2(),\n",
    "], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    "\n",
    "strong_augmentations = A.Compose(\n",
    "        [\n",
    "            A.Resize(SIZE[0], SIZE[1]),\n",
    "            A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.8),\n",
    "            A.GaussianBlur(blur_limit=(3, 7), sigma_limit=(0.1, 2.0), p=0.5),\n",
    "            A.CoarseDropout(num_holes_range=(3, 3), hole_height_range=(0.05, 0.1),\n",
    "                             hole_width_range=(0.05, 0.1), p=0.5),\n",
    "            A.Lambda(image=scale_to_01), \n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        bbox_params=A.BboxParams(format='pascal_voc')\n",
    "    )\n",
    "\n",
    "test_transforms = A.Compose([\n",
    "    A.Resize(SIZE[0], SIZE[1]),\n",
    "    A.Lambda(image=scale_to_01), \n",
    "    ToTensorV2(), \n",
    "], bbox_params=A.BboxParams(format='pascal_voc'))\n",
    "\n",
    "\n",
    "dt_train_labeled = get_dataloader(\"trainval\", \"2007\", BATCH_SIZE, transform=weak_augmentations)\n",
    "dt_train_unlabeled_weakaug = get_dataloader(\"trainval\", \"2012\", BATCH_SIZE, transform=weak_augmentations) \n",
    "dt_train_unlabeled_strongaug = get_dataloader(\"trainval\", \"2012\", BATCH_SIZE, transform=strong_augmentations)\n",
    "dt_test = get_dataloader(\"test\", \"2007\", BATCH_SIZE, transform=test_transforms, shuffle=False)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6da2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history, save_dir=None, filename=\"loss_plot.png\"):\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    epochs = range(1, len(history[\"total\"]) + 1)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "\n",
    "    for comp in METRIC_SUPERVISED:\n",
    "        plt.plot(epochs, history[f\"{comp}_supervised\"], label=f\"Train {comp}_supervised\", linewidth=2)\n",
    "    for comp in METRICS_UNSUPERVISED:\n",
    "        plt.plot(epochs, history[f\"{comp}_unsupervised\"], label=f\"Train {comp}_unsupervised\", linewidth=2)\n",
    "        \n",
    "    plt.plot(epochs, history[\"total\"], label=\"Train total\", linewidth=2)\n",
    "\n",
    "    plt.title(\"Training Loss Components Over Epochs\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Save to disk\n",
    "    if save_dir is not None:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        out_path = os.path.join(save_dir, f\"{filename}_{len(history[\"total\"]) + 1}.png\")\n",
    "        plt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(f\"[INFO] Plot saved to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ecaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint_path, optimizer=None, device='cuda'):\n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {checkpoint_path}\")\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    model = get_model(device=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model weights loaded from {checkpoint_path}\")\n",
    "\n",
    "    if optimizer and 'optimizer_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(f\"Optimizer state loaded from {checkpoint_path}\")\n",
    "\n",
    "    epoch = checkpoint.get('epoch', 0)\n",
    "    print(f\"Resuming from epoch {epoch}\")\n",
    "    return model, optimizer, epoch\n",
    "\n",
    "def train_burn_in(model, optimizer, dt_train_labeled, device):\n",
    "    model.train()\n",
    "    train_batches = 0\n",
    "    history = {key : 0 for key in METRIC_SUPERVISED}\n",
    "\n",
    "    for images, targets in tqdm(dt_train_labeled, desc=\"Training\"):\n",
    "        # if train_batches == 5: break\n",
    "        for target in targets:\n",
    "            target[\"boxes\"] = target[\"boxes\"].to(device)\n",
    "            target[\"labels\"] = target[\"labels\"].to(device)\n",
    "        images = images.to(device)\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        for k, v in loss_dict.items():\n",
    "            history[k] += v.item()\n",
    "\n",
    "        history[\"total\"] += loss.item()\n",
    "        train_batches += 1\n",
    "    for key in history:\n",
    "        history[key] = history[key] / train_batches\n",
    "    return history\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, path):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"Checkpoint saved at {path}\")\n",
    "\n",
    "\n",
    "def pipeline_burn_in(epochs, dt_train_labeled, device, checkpoint_every):\n",
    "\n",
    "    model = get_model(device=device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    history = {key : [] for key in METRIC_SUPERVISED}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n==================== Epoch {epoch+1}/{epochs} ====================\\n\")\n",
    "        train_history = train_burn_in(model, optimizer, dt_train_labeled, device)\n",
    "        lr_scheduler.step(train_history[\"total\"])\n",
    "        for key, val in train_history.items():\n",
    "            history[key].append(val)\n",
    "        plot_losses(history, save_dir=\"results\")\n",
    "        if (epoch + 1) % checkpoint_every == 0 or (epoch + 1) == epochs:\n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, f\"checkpoint_epoch_{epoch+1}.pth\")\n",
    "            save_checkpoint(model, optimizer, epoch + 1, checkpoint_path)\n",
    "\n",
    "# pipeline_burn_in(50, dt_train_labeled, device, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a294f685",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(dt_train_unlabeled_weakaug))\n",
    "from torchvision.ops import batched_nms\n",
    "NMS_IOU = 0.5\n",
    "def generate_pseudo_labels(model : torch.nn.Module, images : torch.Tensor, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        outputs = model(images, None)\n",
    "        for output in outputs:\n",
    "            boxes  = output[\"boxes\"]\n",
    "            labels = output[\"labels\"]\n",
    "            scores = output[\"scores\"]\n",
    "\n",
    "            keep_nms = batched_nms(\n",
    "                boxes, scores, labels,\n",
    "                iou_threshold=NMS_IOU\n",
    "            )\n",
    "            boxes  = boxes[keep_nms]\n",
    "            labels = labels[keep_nms]\n",
    "            scores = scores[keep_nms]\n",
    "\n",
    "            boxes_to_keep = scores > CONFIDENCE_THRESHOLD        \n",
    "            boxes  = boxes[boxes_to_keep]\n",
    "            labels = labels[boxes_to_keep]\n",
    "            scores = scores[boxes_to_keep]\n",
    "\n",
    "            output[\"boxes\"]  = boxes\n",
    "            output[\"labels\"] = labels\n",
    "            output[\"scores\"] = scores\n",
    "        return outputs       \n",
    "    \n",
    "# model, optimizer, epoch = load_checkpoint(checkpoint_path=checkpoint_path, optimizer=None, device=device)\n",
    "# generate_pseudo_labels(model, images, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406f6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_semi_supervised_one_epoch(teacher : RobustEMA, student, optimizer, dt_labeled, dt_weak, dt_strong):\n",
    "    student.train()\n",
    "    train_batches = 0\n",
    "    history = {}\n",
    "    for key in METRIC_SUPERVISED:\n",
    "        history[f\"{key}_supervised\"] = 0\n",
    "    for key in METRICS_UNSUPERVISED:\n",
    "        history[f\"{key}_unsupervised\"] = 0\n",
    "    history[\"total\"] = 0\n",
    "\n",
    "    for (img_labeled, targets_labeled), (img_weak, _), (img_strong, _) in zip(dt_labeled, dt_weak, dt_strong):\n",
    "        if train_batches == 5: break\n",
    "        # SHOULD REPLACE THE TRANSFORMATION OF HORIZONTAL FLIP WITH SOMETHING PHOTOMETRIC\n",
    "        weak_targets = generate_pseudo_labels(teacher.ema, img_weak, device)\n",
    "        \n",
    "        for target in weak_targets:\n",
    "            target[\"boxes\"] = target[\"boxes\"].to(device)\n",
    "            target[\"labels\"] = target[\"labels\"].to(device)\n",
    "        img_strong = img_strong.to(device)\n",
    "        loss_dict_unsupervised = student(img_strong, weak_targets)\n",
    "\n",
    "        for target in targets_labeled:\n",
    "            target[\"boxes\"] = target[\"boxes\"].to(device)\n",
    "            target[\"labels\"] = target[\"labels\"].to(device)\n",
    "        img_labeled = img_labeled.to(device)\n",
    "        loss_dict_supervised = student(img_labeled, targets_labeled)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = sum(loss_dict_supervised.values()) + LAMBDA_UNSUPERVISED * (loss_dict_unsupervised[\"loss_classifier\"] + loss_dict_unsupervised[\"loss_objectness\"])\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        teacher.update(student)\n",
    "        for k in METRICS_UNSUPERVISED:\n",
    "            history[f\"{k}_unsupervised\"] += loss_dict_unsupervised[k].item()\n",
    "        for k in METRIC_SUPERVISED:\n",
    "            history[f\"{k}_supervised\"] += loss_dict_supervised[k].item()\n",
    "\n",
    "        history[\"total\"] += loss.item()\n",
    "        train_batches += 1\n",
    "    for key in history:\n",
    "        history[key] = history[key] / train_batches\n",
    "    return history\n",
    "\n",
    "\n",
    "def run_semi_supervised_pipeline(checkpoint_path, epochs, dt_labeled, dt_weak, dt_strong, dt_test):\n",
    "    student, _, _ = load_checkpoint(checkpoint_path=checkpoint_path, optimizer=None, device=device)\n",
    "    teacher = RobustEMA(student)\n",
    "    optimizer = torch.optim.SGD(student.parameters(), lr=1e-3, momentum=0.9)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    history = {}\n",
    "    for key in METRIC_SUPERVISED:\n",
    "        history[f\"{key}_supervised\"] = []\n",
    "    for key in METRICS_UNSUPERVISED:\n",
    "        history[f\"{key}_unsupervised\"] = []\n",
    "    history[\"total\"] = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n==================== Epoch {epoch+1}/{epochs} ====================\\n\")\n",
    "        train_history = train_semi_supervised_one_epoch(teacher, student, optimizer, dt_labeled, dt_weak, dt_strong)\n",
    "        lr_scheduler.step(train_history[\"total\"])\n",
    "        for key, val in train_history.items():\n",
    "            history[key].append(val)\n",
    "        # print(history)\n",
    "        plot_losses(history, save_dir=\"results\")\n",
    "\n",
    "checkpoint_path=\"checkpoints/checkpoint_epoch_42.pth\"\n",
    "run_semi_supervised_pipeline(checkpoint_path, 50, dt_train_labeled, dt_train_unlabeled_weakaug, dt_train_unlabeled_strongaug, dt_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
